\section{Introduction}
Stock assessment models are a key element of modern fisheries management \parencite{punt2020model}. There are various definitions of stock assessment, of which we prefer the following:\textit{The description of the characteristics of a 'stock' so that its biological reaction to being exploited can be rationally predicted and the predictions tested}, (Holt pers comm.), since this explicitly recognises that the main aim of a stock assessment is to provide the information necessary for the sustainable management of fisheries resources. Stock assessment models are generally used to derive probabilistic representation of the stock status and forecast to be used to provide advice. While it is important that stock assessment models are able to explain the data, they should also generalise the inference to the fundamental biological and statistical processes of the concerned resources. Therefore, there is the need to validate models using their prediction skill, since accuracy and precision of predictions relate to the validity of the model, the information in the data, bur also to how far ahead we wish to predict. 

In stock assessment, however, most goodness of fit diagnostics are based either on model residuals or retrospective analysis obtained from fits to historical observations (Reference to ICES here). The use of different model structures with different data implies that it is difficult to select models using conventional statistical metrics such as AIC (Reference).  Furthermore, stock assessment tends to focus on model selection, which searches for the most suitable model within a family, and hypothesis testing, which examines if the model structure can be reduced. In contrast model validation looks forward by examining if a model should be modified or extended. (NOT entirely true, when testing different hypothesis, implicitly we assume that the model might need to be modified or extended as well). 

[DF: I think the above paragraph needs a bit reorganizing, suggested below:
The stock assessment development often focuses on model selection and validation, which involves searching for the most suitable models and examining if a particular model structure should be modified, reduced, or extended. To date, most model validation are undertaken through likelihood-based approaches such as hypothesis testing, goodness of fit diagnostics, and retrospective analysis obtained from fits to historical analysis. However,  the use of different model structures with different data or statistical distribution assumption implied at is is difficult to compare models using conventional likelihood-based statistical metrics such as AIC]

Model validation is important in many fields, e.g. in energy and climate models, as it increases confidence in the outputs of a model and leads to an increase in trust amongst the public, stake and asset-holders and policy makers \parencite{kell2019optimising}. For models to be valid they must satisfy four prerequisites \parencite{hodges1992you}: the situation being modelled must (i) be observable and measurable, (ii) it must be possible to collect sufficient data informative about it, (iii) exhibit constancy of structure in time, and (iv) exhibit constancy across variations in conditions not specified in the model. The first two prerequisites should be straight forward, but many stock assessments depend on fisheries dependent data rather than direct scientific observations. For example highly migratory stocks fished in areas beyond national jurisdiction (ABNJ) commonly employ indices of abundance based on commercial catch rates[Suggested changes: The first two prerequisites should be strait forward. For example, highly migratory stocks fished in areas beyond national jurisdiction (ABNJ) commonly employ indices of abundance based on commercial catch rates although there are often concerns that they are based on fisheries dependent data rather than direct scientific observations] (Is this a real issue in order to fullfill i) and ii)? Are not commercial CPUE observable and measurable?) . Prerequisite (iii) ensures that the model has predictive skill for the same conditions under which the validation tests were conducted, while prerequisite (iv) ensures that the model will still be valid for conditions that differ from those in the validation tests, i.e. can be used to set robust management advice.

[DF: suggested combining the below two paragraphs as:
In retrospective analysis, data from the most recent years are sequentially removed and the model refitted to check the stability of past model estimates \parencite{hurtado2014looking}. Stability of historical estimates, however, can be achieved at the expense of the accuracy and precision of forecasts or at the expense of mis-fitting the data. The absence of retrospective patterns, while reassuring, is not sufficient as it is not prerequisite to fulfill i), which implies that it is not possible to validate models based on model outputs only [CLARIFY]. Therefore, we extend retrospective analysis by projecting forward for the reported catch over the years removed. This allows prediction ability to be evaluated. We also conduct model-free hindcasts to compare observations (i.e. CPUE from commercial fleets or surveys) with model estimates of the same quantities (pseudo observations) in order to evaluate model prediction skill. The use of model prediction skill allows different model structures and data components to be compared in order to identify potential model mis-specification and data conflicts. 
]


In retrospective analysis, data from the most recent years are sequentially removed and the model refitted to check the stability of past model estimates \parencite{hurtado2014looking}. Stability of historical estimates, however, can be achieved at the expense of the accuracy and precision of forecasts or at the expense of mis-fitting the data. Therefore, we extend retrospective analysis by projecting forward for the reported catch over the years removed. This allows prediction ability to be evaluated. 

The absence of retrospective patterns, while reassuring, is not sufficient as it is not prerequisite to fulfill i), which implies that it is not possible to validate models based on model outputs only [CLARIFY]. Therefore, we also conduct model-free hindcasts to compare observations (i.e. CPUE from commercial fleets or surveys) with model estimates of the same quantities in order to evaluate model prediction skill. The use of model prediction skill allows different model structures and data components to be compared in order to identify potential model mis-specification and data conflicts. 

We compare three model structures used in the assessment of Indian Ocean yellowfin tuna stock, namely a full integrated model (SS3), an age structured production model (ASPM), and a biomass dynamic model (JABBA). We do this by firstly using model-based quantities and then using peudo observations with multiple metrics. We discuss hypothesis testing, model selection and model validation, then make recommendations for the bench-marking of stock assessment methods and discuss the implications for Management Strategy Evaluations (MSEs). In particular, for the weighting of alternative Operating Models, modelling process error, and the development of Observation Error Models.   

[DF - I suggest not mentioning about MSE, weighting of OM, process error, etc in the paragraph above as they appears only in the Discussion section, and they are not part of the analysis conducted]
