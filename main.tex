\documentclass[a4paper]{article}

%% Language and font encodings
\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage[T1]{fontenc}

%% Sets page size and margins
\usepackage[a4paper,top=3cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

%% Useful packages
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}

\usepackage[authoryear]{natbib}

%% My definition
\newcommand{\toshi}{\textcolor{blue}}
\newcommand{\laurie}{\textcolor{red}}
\newcommand{\rishi}{\textcolor{green}}
\newcommand{\iago}{\textcolor{purple}}

\newcommand{\disp}{\displaystyle}

\usepackage{enumitem}
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}

\usepackage{color}
\definecolor{darkgreen}{rgb}{0.0, 0.5, 0.0}
\definecolor{darkred}{rgb}{0.7, 0.11, 0.11}
\definecolor{darkblue}{rgb}{0,0,0.5}
\definecolor{shadecolor}{rgb}{1,1,0.95}
\definecolor{shade}{rgb}{1,1,0.95}
\definecolor{coilin}{rgb}{1,0,1}

\title{Is It You or Your Model Talking}
\author{Laurence Kell, Toshihide Kitakado, Rishi Sharma, Henning Winker, \\ Iago Mosqueira, Dan Fu, Max Cardinale, ...}

\begin{document}


\section*{ICES Background}

Observational data are imperfect, and predictive models based on those data represent simplifications of how some aspect of the world works. Thus, in the fisheries advisory process (e.g., development of catch advice using stock assessments, evaluation of management strategies), it is clear that our analyses are fraught with uncertainty, stemming from uncertainty in the input data (observations) or from the \laurie{structure (degree of simplification and validity of assumptions)} of the methods and models employed. Input uncertainty is easier to measure using standard statistical procedures, and to address through improvements to survey designs, sampling schemes, and statistical methods. \laurie{Structural uncertainties however, are more intangible as they often represent “known unknowns” - i.e., we know there are limitations to the methods and models, but it is difficult to describe and measure them without comprehensive analyses, such as simulation testing or cross-validation}.

With the development of more advanced analytical frameworks that support implementation of  machine learning, artificial intelligence, and ensemble modeling, we invite fisheries scientists to a session to present advances in \laurie{identifying, quantifying and dealing with structural uncertainties in the fisheries science advisory process.}

We invite contributions on the following themes:

\begin{itemize}
    \item \laurie{Uncertainties throughout the stock assessment and management strategy evaluation process}
    \item \laurie{Identification and testing of plausible structural hypotheses, and structural uncertainties}
    \item \laurie{Sensitivity analysis}
    \item \laurie{Model ensembles (within and between models) see previous \href{ https://www.fisheries.noaa.gov/national/population-assessments/noaa-fisheries-13th-national-stock-assessment-workshop-report-released}{NOAA workshop} }
    \item \laurie{Gaps and needs for developing ensembles}
    \item \laurie{Evaluating trade-offs between one versus multiple models}
    \item \laurie{Combining and communicating results across ensemble members and stakeholders}
\end{itemize}


\maketitle
 
\section*{Still To Do}
\begin{itemize}
   \item Review sections in include files, make sure the text is complete, not redundant,  and fits into the overall text.
   \item Finalise results by adding tables if they help.
   \item Discussion based on elements that are in the outline.
   \item Mention process error \& recruitment patterns then do in Surplus Production paper  
   \item Get Max and Dan to review and submit to IJMS by June 30th
\end{itemize}

 \section*{Outline}
\begin{itemize}
    
    %\item The provision of fisheries management advice requires the assessment of stock status relative to reference points, the prediction of the response of a stock to management, and checking that predictions are consistent with reality (Holt pers com.) 
    
    %\item The Precautionary Approach requires the development of advice that is robust to uncertainty. Therefore, often when conducting stock assessments multiple models with different structures and datasets, are used to explore uncertainty. This means, however, that it is difficult to compare models using conventional metrics such as AIC. 
    
    \item  We  therefore compare SS, SS-ASPM, and Jabba assessments for Indian Ocean yellowfin tuna  using multiple metrics, and discuss hypothesis testing, model selection and model validation. We make recommendations for the bench-marking of stock assessment methods and discuss the consequences for MSE. In particular weighting of alternative Operating Models, modelling process error, and the development of Observation Error Models.   

    
   \item We therefore predict forward the retrospective analyses and compare model predictions with historical estimates.The absence of retrospective patterns, however, while reassuring is not sufficient as it is not possible to validate models based on model outputs. We therefore conduct model free hindcasts to compare observations with model estimates. 
   
   \item The use of metrics based on prediction skill allows different data components and model to be compared in order to explore data conflicts and potential model misspecification. The accuracy and precision of predictions depend on the validity of the model, the information in the data, and how far ahead predictions are required. 
    
\end{itemize}

\newpage
\tableofcontents 



\newpage    
\begin{abstract}
    Evaluating how well the model fits data has been receiving much attention in fisheries science, both in terms of goodness-of-fit and retrospectively. This however merely tells us how well we can describe the past, yet little how well we can predict the future under alternative management actions. In this paper, we revisit the concepts behind hindcasting cross-validation (hcxval) as an important model-free validation tool for predictive modelling. Together with conventional residual diagnostics and retrospective analysis, we apply hcxval to three examples of alternative candidate models using the recent Indian Ocean yellowfin tuna assessment as a case study. 
    These models comprise the 2019 spatially structured reference model implemented in Stock Synthesis (ss-ref), a deterministic age-structured production model (ss-aspm) of ss-ref and a simplied spatially aggregated stochastic surplus production model implemented in the 'JABBA' package. To assess prediction skill, we computed the Mean-Absolute-Scaled-Error (MASE), which, unlike e.g. Aikaike's Information Criterion, enables to compare across different models fitted to different data. The best MASE values (MASE < 1) were determined for ss-asem, which indicates that recruitment deviations in ss-ref were poorly estimated due to no or limited information in the 'noisy' length composition data. By contrast, the area effects retained in ss-aspm best explained its superior prediction skill compared to the spatially aggregated jabba model. We suggest that one-step ahead predictions are efficient for detecting overfitting and for model validation in general, but for future quota advice the forecast horizon should preferably at least match the assessment interval to ultimately increase confidence in the model-based scientific advice by stakeholders, managers and policy makers.
\end{abstract}

\newpage
\section{Introduction: Laurie + Toshi}

To provide fisheries management advice requires predicting the response of a stock to management and checking that the predictions are consistent with reality (pers. com. Sidney Holt). The accuracy and precision of predictions depend on the validity of the model, the information in the data, and how far ahead we wish to predict. The Precautionary Approach requires the development of advice that is robust to uncertainty. Therefore, when conducting stock assessments multiple models with different structures and datasets, are often used to explore uncertainty. 

In stock assessment, however, most goodness of fit diagnostic are either based on model residuals or retrospective analysis obtained from fits to historical observations. While the use of different modelling frameworks with different data needs means that it is difficult to select models using conventional metrics such as AIC.  As well as model selection which searches for the most suitable model within a family, and hypothesis testing which examines if the model structure can be reduced, model validation is also required. Validation examines if a model should be modified or extended and is complementary to model selection and hypothesis testing.

Model validation is important in many fields, e.g. in energy and climate models, as it increases confidence in the outputs of a model and leads to an increase in trust amongst the public, stake and asset-holders and policy makers \citep{kell2019optimising}. For models to be valid they must satisfy four prerequisites \cite{hodges1992you}: the situation being modelled must (i) be observable and measurable, (ii) it must be possible to collect sufficient data informative about it, (iii) exhibit constancy of structure in time, and (iv) exhibit constancy across variations in conditions not specified in the model. The first two prerequisites should be straight forward, but many stock assessments depend on fisheries dependent data rather than observation. For example highly migratory stocks fished in areas beyond national jurisdiction (ABNJ) commonly employ indices of abundance based on commercial catch rates. Prerequisite iii ensures that the model has predictive skill for the same conditions under which the validation tests were conducted, while prerequisite iv ensures that the model will still be valid for conditions that differ from those in the validation tests, i.e. can be used to set robust management advice. 

 We  therefore compare SS, SS-ASPM, and Jabba assessments for Indian Ocean yellowfin tuna  using multiple metrics, in order to help improve bench-marking of stock assessment methods.
 
    
\section{Material and Methods}

Retrospective analysis  \citep{hurtado2014looking} is commonly used to evaluate the stability of stock assessment estimates from alternative models. Observations are sequentially removed from the terminal year, the model is then refitted to the truncated series and the difference between between estimates from the full and truncated time-series compared using the relative error (RE, a measure of bias). The use of model based quantities, however, means that bias can not actually be quantified. A reduction in both RE and mean squared error (MSE, a measure of variance) can be achieved by shrinking terminal estimates towards recent historical values, at the expense of prediction skill. The absence of retrospective patterns, therefore, while reassuring is not sufficient for model validation.

Therefore to compare the different family of models we extend retrospective analysis to conduct a hindcast. Where a hindcast or a backtest is a way of testing a model based on past events by comparing outputs match with known results. We therefore extend retrospective analysis by adding an additional step of projecting over the truncated years. We then conduct a model-free hindcast \cite{kell2016xval} to estimate prediction skill, defined as any measure of accuracy of a forecasted value to the actual (i.e. observed) value that is not known by the model \citep{glickman2000glossary}. 

\subsection{Assessment Methods: Rishi}
\input{assess.tex}

\subsection{Materials: Iago}
\input{material.tex}

\subsection{Hindcast: Laurie then Henning}

Hindcasting like traditional retrospective analysis involves fitting a model using a tailcutting procedure, where data are deleted sequentially for $n$ years, i.e. from the last year $y$ through to $y−n$, the additional step in the hindcast is that then the data from year 1 to $y−n−1$ are used to make predictions of what will happen in years $y−n$ to $y$.

\begin{algorithm}[!ht]
\begin{algorithmic}[1]

\State Conduct assessment for all years, i.e. $t=n$
\For ~ $t$ = n to n-20
\State Remove all $X_{i,t}$ for $t$ to n
\State Run model
\State Estimate $\hat{X}_{i,t}$ to $\hat{X}_{i,n}$ 
\EndFor
\caption{Hindcast~\citep{kell2016xval}}
\label{Hindcast}
\end{algorithmic}
\end{algorithm}

Hindcasting can be performed for model based quantities (e.g. SSB and F) or model-free (e.g. observations of CPUE). The former helps check the stability of stock assessment advice, while the later provides an objective way to validate models based on prediction skill and to test for over-fitting. The later is important since analysis of residuals is a common way to determine a model’s goodness-of-fit \citep{Cox1968general}, since  non-random patterns in the residuals may indicate model misspecification, serial correlation in sampling/observation error, or heteroscedasticity. When inspecting residuals, however, there is a danger of hypothesis fishing, i.e. choosing a scenario retrospectively retrospect, while if multiple true hypotheses are tested it is likely that some of them will be rejected. Therefore it is valuable to reserve part of the data for validation, so that a pattern’s significance is not tested on the same data set which suggested the pattern \citep{thygesen2017validation}. For this reason we conduct a model-free hindcast.

Since assessment cycles are typically for three years  with advice \citep{fricker2013three} we projected the truncated estimates for 3 years. When conducting the hindcast it is assumed that modelled variables are observable, processes exhibit constancy of structure in time, including those not specified in the model, and that collection of accurate and sufficient data is possible \citep{hodges1992you}.

\subsubsection{Model-Based}
\input{model-based.tex}

\subsubsection{Model-Free}
\input{model-free.tex}

\section{Results: Rishi}

The retrospective analysis for one and three step ahead prediction is shown in figure \ref{fig:retro} for $SSB:B_{MSY}$ and $F:F_{MSY}$ (SSB & F for SS and ASPM and exploitable biomass and harvest rate for JABBA). Even for a one year ahead prediction, the base case and JABBA assessments start to diverge from the assessment that included all years. The pattern gets worse for the three year projections. Table \tref{tab:retro} shows the estimates of Monh's $\rho$, RMSE, and MASE. This allows bias, variance and model-based prediction skill to be compared.

ASPM values are relatively unbiased with low variance and good prediction skill. For SS, however, there is over and underestimation of SSB and F respectively. For JABBA a strong negative retrospective pattern is seen in F, a negative bias is also seen in biomass.

The residuals from the model fits to the indices of abundance are shown in Figure~\ref{fig:runs}, the background indicates whether they passed (green) or failed (red) the runs tests. In general the CPUE for Area 2 and Area 4 perform poorly for both the base case and the JABBA models. 1st quarter indices also perform poorly compared to the other quarters (quarter 3 for CPUE 1 is also poorly predicted). As indicated the green shows that the runs test passes for most of the ASPM model data, and a lot more red indicators exist for the other two models.  

The results from the model-free hindcasts are shown in Figures~\ref{fig:hy} and \ref{fig:hy3} for one and three year ahead predictions respectively. CPUE's from area 2 and 4 perform poorly as in the previous figures for the base case and JABBA. The ASPM also performs poorly for Area 2 in the 3 year hindcast projections. Model fits for 3 years ahead for both JABBA and the base case perform poorly. Area 2 also fails for ASPM in the 3 year projections. As a bulk of the catch comes from Area 1 and 2, Area 2 index of abundance is important to predict. In addition Area 4 in the eastern part of the Indian Ocean perform poorly with all models in the three year hindcast exercise.   

The fits are summarised in Figures~\ref{fig:td} in the form of Taylor diagrams \citep{taylor2001summarizing}. 

%In the one year hindcast, all models other than area 2 indices had a high RMSE, high sigma, and low correlation (left panel). Other models seem to have a high overlap (lower left cluster indicating a high correlation, and low RMSE and low sigma). As we go to a three year hindcast, all models seem to perform poorly, other than the ASPM. Even, the ASPM, appears to do poorly with the 2nd CPUE. the correlation gets worse for all models for CPUE 1 & 2, though the RMSE and sigma remains low.  

Table \ref{tab:retro} and  \ref{tab:proj} show Mohn's $\rho$ for the retrospective analysis and retrospective with projection, Table \label{tab:rmse} shows the values of RMSE and \label{tab:mase} MASE for the model free hindcast.

\begin{description}
    \item{Retrospective analysis for F/FMSY and B/BMSY}
  
    \begin{itemize}
        \item Figure \ref{fig:retro} and Table \ref{tab:retro} summarise the retrospective analysis, taking 0.2 and -0.15 as the cut off points for accepting an assessment all assessments pass. 
        \item The strongest retrospective patterns are seen for SS, where F is negatively biased, and although the value of Mohn's $\rho$ is low for SSB a strong pattern is seen with underestimates followed by overestimates. 
        \item Although recent Jabba estimates are unbiased historical estimates of F and Biomass are negatively biased.
        \item Jabba estimates are problematic as it appears that even if F<FMSY the stock will decline below BMSY
        \item ASPM estimates appears to have little bias
    \end{itemize}
    \item{Retrospective analysis and projections for F/FMSY and B/BMSY} 
    \begin{itemize}
      \item Figure \ref{fig:proj} and Table \ref{tab:proj} summarise the retrospective analysis combinded with a three prediction, again taking the cut off as 0.2 to -0.15, both SS and Jabba fail.
      \item ASPM appears not to show patterns in the projections
        \item ASPM performs best
        \item Survey 2 performs poorly across all models
        \item It appears that the length compositions add noise (SS), and that area effects (JABBA) are important.
        \item However these is no objective way to choose an assessment based on retrospective analysis as the best model would always be B/MSY = 1 
   \end{itemize}
   \item{Residuals}
    \begin{itemize}
        \item Figure \ref{fig:runs} summarises the residuals, in the form of runs test; green backgrounds denote a pass. Only two indices pass for all models Survey 1 quarter 1 and Survey 3 quarter 4
        \item Over half of the indices fail for Jabba, half for SS while for ASPM the majority pass. This indicates strong data conflicts while  failure of the runs test may explain the retrospective patterns.
   \end{itemize}
     \item{Model-free cross-validation} confirms the relative performance of the models
    \begin{itemize}
        \item Figures \ref{fig:hy1} and \ref{fig:hy3} shows that SS performs poorly, possibly because length compositions can only be explained by variations in year-class strengths, and projections predict large increase in biomass.
        \item RMSE is difficult to interpret (Table \ref{tab:rmse}), MASE easier (Table \ref{tab:mase}).
        \item Taylor diagram, shows that there is a big difference between model residuals and prediction residuals. Figure \ref{fig:td} summarises the historical model fits survey 4 performs the best, i.e. has high correlation for all models, while survey 3 performs poorly.
        \item However, when three step ahead projections are considered (Figure \ref{fig:tdhat} ) survey 4 perform the best for ASPM, although the correlation is reduced. SS has high RMSE, poor correlation and high  variance.  
   \end{itemize}
 \end{description}

\section{Discussion: All}
\input{discussion.tex}

\section{Conclusions: All}

Primary objectives were:
\begin{enumerate}
    \item In fisheries unlike other fields we try to account for the past but not for the future. Here we propose a way to assess model predictive performance and to account for alternative models within a common diagnostic framework.  
    \item Unifying platform for evaluating across models
    \item Advantage of MASE : What are the new properties of this stat
    \item Consequences for stock assessment benchmarking
    \item Consequences for uncertainty, risk and MSE.
    \item Next steps
\end{enumerate}

\section{Appendix: Summary Metrics}
\input{metrics_TK.tex}

\bibliography{main.bib}
\bibliographystyle{apalike}

\section{Tables: Henning}

\iffalse
\begin{table}[ht]
\caption{Mohn's $\rho$ for retrospective analysis.}  
\begin{center}
\label{tab:retro-}
\begin{tabular}{|cccc|}
\hline
	\tiny Method	& {\tiny $\Quantity$}  & {\tiny Retrospective} & {\tiny Projection} \\ 
\hline\hline
{\tiny SSB          } & {\tiny SS} 	     & {\tiny   NA}  & {\tiny NA}      \\
{\tiny SSB          } & {\tiny ASPM} 	 & {\tiny   NA}  & {\tiny NA}      \\
{\tiny SSB          } & {\tiny JABBA} 	 & {\tiny   NA}  & {\tiny NA}      \\
{\tiny F            } & {\tiny SS} 	     & {\tiny   NA}  & {\tiny NA}      \\
{\tiny F            } & {\tiny ASPM} 	 & {\tiny   NA}  & {\tiny NA}      \\
{\tiny Harvest Rate } & {\tiny JABBA} 	 & {\tiny   NA}  & {\tiny NA}      \\
\hline
\end{tabular}
\end{center}
\end{table}
\fi

\begin{table}[ht]
\caption{Mohn's $\rho$ for retrospective analysis.}  
\label{tab:retro}
\centering
\begin{tabular}{rllr}
  \hline
 & run & variable & V1 \\ 
  \hline
  1 & aspm & stock & -0.03 \\ 
  2 & aspm & harvest & 0.03 \\ 
  3 & base & stock & 0.04 \\ 
  4 & base & harvest & -0.15 \\ 
  5 & jabba & stock & -0.09 \\ 
  6 & jabba & harvest & -0.09 \\ 
   \hline
\end{tabular}
\end{table}


\begin{table}[ht]
\caption{Mohn's $\rho$ for retrospective analysis with three year projection.}  
\label{tab:proj}
\centering
\begin{tabular}{rllr}
  \hline
 & run & variable & V1 \\ 
  \hline
  1 & aspm & stock & -0.09 \\ 
  2 & aspm & harvest & 0.08 \\ 
  3 & base & stock & 0.32 \\ 
  4 & base & harvest & -0.24 \\ 
  5 & jabba & stock & -0.21 \\ 
  6 & jabba & harvest & -0.28 \\
   \hline
\end{tabular}
\end{table}



\begin{table}[ht]
\caption{RMSE.}  
\label{tab:rmse}
\centering
\begin{tabular}{rlrrrr}
  \hline
 & name & quarter & SS & ASPM & JABBA \\ 
  \hline
  1 & CPUE 1 & 1.00 & 0.37 & 0.24 & 0.25 \\ 
  2 & CPUE 1 & 2.00 & 0.28 & 0.24 & 0.26 \\ 
  3 & CPUE 1 & 3.00 & 0.47 & 0.45 & 0.29 \\ 
  4 & CPUE 1 & 4.00 & 0.29 & 0.14 & 0.24 \\ 
  5 & CPUE 2 & 1.00 & 0.36 & 0.34 & 0.43 \\ 
  6 & CPUE 2 & 2.00 & 0.64 & 0.62 & 0.50 \\ 
  7 & CPUE 2 & 3.00 & 0.30 & 0.29 & 0.23 \\ 
  8 & CPUE 2 & 4.00 & 0.42 & 0.42 & 0.38 \\ 
  9 & CPUE 3 & 1.00 & 0.28 & 0.19 & 0.21 \\ 
  10 & CPUE 3 & 2.00 & 0.34 & 0.31 & 0.38 \\ 
  11 & CPUE 3 & 3.00 & 0.22 & 0.22 & 0.30 \\ 
  12 & CPUE 3 & 4.00 & 0.37 & 0.36 & 0.40 \\ 
  13 & CPUE 4 & 1.00 & 0.37 & 0.15 & 0.41 \\ 
  14 & CPUE 4 & 2.00 & 0.45 & 0.30 & 0.53 \\ 
  15 & CPUE 4 & 3.00 & 0.49 & 0.25 & 0.43 \\ 
  16 & CPUE 4 & 4.00 & 0.47 & 0.18 & 0.48 \\ 
   \hline
\end{tabular}
\end{table}

\begin{table}[ht]
\caption{MASE.}  
\label{tab:mase}
\centering
\begin{tabular}{rlrrrr}
  \hline
 & name & quarter & SS & ASPM & JABBA \\ 
  \hline
  1 & CPUE 1 & 1.00 & 0.94 & 0.53 & 0.77 \\ 
  2 & CPUE 1 & 2.00 & 0.63 & 0.67 & 0.96 \\ 
  3 & CPUE 1 & 3.00 & 1.23 & 1.17 & 0.85 \\ 
  4 & CPUE 1 & 4.00 & 0.82 & 0.45 & 0.74 \\ 
  5 & CPUE 2 & 1.00 & 1.52 & 1.73 & 2.41 \\ 
  6 & CPUE 2 & 2.00 & 2.11 & 2.10 & 1.56 \\ 
  7 & CPUE 2 & 3.00 & 0.95 & 0.82 & 0.83 \\ 
  8 & CPUE 2 & 4.00 & 1.33 & 1.65 & 1.26 \\ 
  9 & CPUE 3 & 1.00 & 0.86 & 0.57 & 0.88 \\ 
  10 & CPUE 3 & 2.00 & 0.92 & 0.81 & 0.92 \\ 
  11 & CPUE 3 & 3.00 & 0.93 & 0.76 & 1.22 \\ 
  12 & CPUE 3 & 4.00 & 0.85 & 0.91 & 0.99 \\ 
  13 & CPUE 4 & 1.00 & 2.10 & 0.76 & 3.00 \\ 
  14 & CPUE 4 & 2.00 & 0.91 & 0.63 & 1.14 \\ 
  15 & CPUE 4 & 3.00 & 2.07 & 0.93 & 1.92 \\ 
  16 & CPUE 4 & 4.00 & 3.29 & 1.09 & 3.90 \\ 
   \hline
\end{tabular}
\end{table}

\clearpage
\section{Figures}


\begin{figure*}[htbp]
\centering
\includegraphics[width=6in]{map.png}
\caption{Spatial stratification of the Indian Ocean for the four region assessment model (R1a and R1b were treated as one model region but were retained for the fleet definition). The black arrows represent the configuration of the movement parameterization.  Density contours represent of the dispersal of tag releases (red) and subsequent recaptures from Indian Ocean Regional tuna tagging programme. Green circles represent the distribution of catches from the longline fishery aggregated by 5 longitude * 5 latitude for 1980 – 2017 (max. = 133 770 t).}
\label{fig:map}
\end{figure*}

\begin{figure*}[htbp]
\centering
\includegraphics[width=6in]{ll.png}
\caption{Regional longline CPUE indices included in the 2018 stock assessment. The difference in scales represents the relative distribtuion of longline vulnearable biomass amonst regions.}
\label{fig:ll}
\end{figure*}

\begin{figure*}[htbp]
\centering
\includegraphics[width=6in]{final-retro-all-1.png}
\caption{Retrospective analysis for the three models, points indicate the terminal years, and the think line the assessment using all the data.}
\label{fig:retro}
\end{figure*}

\begin{figure*}[htbp]
\centering
\includegraphics[width=6in]{final-cpue-residual-runs-1.png}
\caption{Residual runs tests for fits to the three models; green background indicates series where runs tests are passed.}
\label{fig:runs}
\end{figure*}

\begin{figure*}[htbp]
\centering
\includegraphics[width=6in]{final-hy-plot-1.png}
\caption{Hindcasts for one step ahead predictions, red dots are the observed CPUE values and lines are the fits with terminal hincast year indicated by a point.}
\label{fig:hy}
\end{figure*}

\begin{figure*}[htbp]
\centering
\includegraphics[width=6in]{final-hy3-plot-1.png}
\caption{Hindcasts for three step ahead predictions, red dots are the observed CPUE values and lines are the fits with terminal hincast year indicated by a point.}
\label{fig:hy3}
\end{figure*}


%\begin{figure*}[htbp]
%\centering
%\includegraphics[width=6in]{final-cpue-prediction-runs-1.png}
%\caption{Runs tests for one step ahead residuals.}
%\label{fig:runshat}
%\end{figure*}

\begin{figure*}[htbp]
\centering
\includegraphics[width=6in]{final-taylor-hy-1-1.png}
\caption{Taylor diagram for one and three year ahead predictions,  summarising the similarity between the observed time series of CPUEs and the predicted relative stock abundance. Each point quantifies how closely predictions match observations, the angle indicates the correlation, the centred root-mean-square error difference between the predicted and observed patterns is proportional to the distance to the point on the x and the contours around this point indicate the RMSE values; the standard deviations of the predictions are proportional to the radial distance from the origin, scaled so the observed pattern has a value of 1. The open circle corresponds to a series which is identical to the reference series. The colours correspond to the model and shape to the survey.)}
\label{fig:td}
\end{figure*}


\end{document}
